import json
from typing import Dict, List, Optional

import tiktoken
from openai import OpenAI

from roboguard.prompts.ablations.base_no_cot import get_system_prompt_no_cot
from roboguard.prompts.ablations.examples_no_cot import get_examples_no_cot
from roboguard.prompts.base import get_system_prompt
from roboguard.prompts.examples import get_examples


class ContextualGrounding:
    def __init__(
        self,
        rules: str,
        use_cot: Optional[bool] = True,
        temperature: Optional[float] = 0.0,
    ) -> None:
        """Initialize the Contextual Grounding model

        Parameters
        ----------
        rules : str
            Rules in natural language
        use_cot : Optional[bool], optional
            Use chain-of-thought reasoning. True by default.
        temperature : Optional[float], optional
            Temperature for root-of-trust LLM, by default 0.0
        """
        self.gpt_client = OpenAI()

        self.token_encoding = tiktoken.get_encoding("cl100k_base")
        self.token_history = []

        self.use_cot = use_cot
        if self.use_cot:
            self.sys_prompt = get_system_prompt()
            self.examples = get_examples()
            self.llm_response_args = {"response_format": {"type": "json_object"}}

        else:
            self.sys_prompt = get_system_prompt_no_cot()
            self.examples = get_examples_no_cot()
            self.llm_response_args = {}

        self.rules = rules
        self.temperature = temperature

    def get_rules(self) -> str:
        return self.rules

    def get_specifications(self, scene_graph: str) -> Dict[str, List[Dict[str, str]]]:
        """Generate LTL constraints given current semantic map

        Parameters
        ----------
        scene_graph : str
            Semantic map via scene graph. See examples.py for formatting

        Returns
        -------
        Dict[str, List[Dict[str, str]]]
            Constraints as follows
            {
                rule: {constraint: ltl_constraint, reasoning: justification for rule},
                ...
            }
        """
        llm_input = (
            self.sys_prompt
            + self.examples
            + [
                {
                    "role": "user",
                    "content": [
                        {"text": f"{self.rules}\n{scene_graph}", "type": "text"}
                    ],
                },
            ]
        )

        n_tokens = len(self.token_encoding.encode(str(llm_input)))
        self.token_history.append(n_tokens)

        response = self.gpt_client.chat.completions.create(
            model="gpt-4o",
            messages=llm_input,
            temperature=self.temperature,
            max_tokens=2925,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
            **self.llm_response_args,
        )

        # The generator provides constraints under the following CoT structure
        # {
        #     <RULE_n>: [(constraint: constraint_1, reasoning: reasoning_1), ...]
        #     ...
        # }
        # replicate that structure for the no CoT version
        generator_response = response.choices[0].message.content
        if self.use_cot:
            formatted_response = json.loads(generator_response)
        else:
            formatted_response = {
                "all_constraints": [{"constraint": generator_response, "reasoning": ""}]
            }

        return formatted_response

    @staticmethod
    def gather_specification_propositions(
        generated_constraints: Dict[str, List[Dict[str, str]]],
    ) -> List[str]:
        """Gather generated specifications into one LTL
        expression.

        Parameters
        ----------
        generated_constraints : Dict[str, List[Dict[str, str]]]
            Specifications generated by the root-of-trust
            LLM's resoning process

        Returns
        -------
        List[str]
            A list of specifications that may be concatenated
            into a single expression.
        """
        constraints = []
        for k, v in generated_constraints.items():
            for constraint in v:
                c = constraint["constraint"]
                if c != "NONE":
                    constraints.append(c)
        if len(constraints) == 0:
            constraints = ["!none"]
        return constraints

    @staticmethod
    def print_specifications(constraints: Dict[str, Dict[str, str]]) -> None:
        for k, v in constraints.items():
            print(k)
            for constraint in v:
                c = constraint["constraint"]
                r = constraint["reasoning"]
                print("", f"constraint: {c}", sep="\t")
                print("", f"reasoning: {r}", sep="\t")
                print("--\n")
